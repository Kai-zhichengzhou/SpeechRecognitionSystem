--- /usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py
+++ /usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py
@@ -1,15 +1,16 @@
 class GELU(Module):
     r"""Applies the Gaussian Error Linear Units function:
 
-    .. math:: \text{GELU}(x) = x * \Phi(x)
-
+    .. math::
+        \text{GELU}(x) = x * \Phi(x)
     where :math:`\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.
 
     Shape:
-        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.
-        - Output: :math:`(*)`, same shape as the input.
+        - Input: :math:`(N, *)` where `*` means, any number of additional
+          dimensions
+        - Output: :math:`(N, *)`, same shape as the input
 
-    .. image:: ../scripts/activation_images/GELU.png
+    .. image:: scripts/activation_images/GELU.png
 
     Examples::
 
@@ -17,6 +18,6 @@
         >>> input = torch.randn(2)
         >>> output = m(input)
     """
-    def forward(self, input: Tensor) -> Tensor:
+    def forward(self, input):
         return F.gelu(input)
 