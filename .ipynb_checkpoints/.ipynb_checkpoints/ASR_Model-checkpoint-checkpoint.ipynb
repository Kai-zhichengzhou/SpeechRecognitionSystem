{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "74a222f9-62c8-4002-9724-f05c934f539c",
     "kernelId": ""
    },
    "id": "oDTX1_lI-ThR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 2,
     "id": "dc50c77d-e127-4ca2-94e6-59ec7189a77e",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import librosa\n",
    "import torch\n",
    "import csv\n",
    "import pandas as pd\n",
    "import SignalProcess\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "execution_count": 7,
     "id": "97a754e7-879f-46c4-b549-7fa93b11806c",
     "kernelId": ""
    },
    "id": "XkBeXK5TQb6H",
    "outputId": "48100ad4-d946-4701-a31b-39b1c95ffda5"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# import pandas as pd\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "execution_count": 3,
     "id": "0c46ca93-6e6d-4fa9-b176-7fc23f0b5986",
     "kernelId": ""
    },
    "id": "rykviCEn-aum",
    "outputId": "5fc61040-6db2-4a5e-d358-a104c9f9390e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "['Hello My First Speech Recognition System']\n",
      "[9, 6, 13, 13, 16, 1, 14, 26, 1, 7, 10, 19, 20, 21, 1, 20, 17, 6, 6, 4, 9, 1, 19, 6, 4, 16, 8, 15, 10, 21, 10, 16, 15, 1, 20, 26, 20, 21, 6, 14]\n",
      "['Hello My First Speech Recognition System']\n",
      "hello my first speech recognition system\n"
     ]
    }
   ],
   "source": [
    "#Make the characters mapping \n",
    "#mapping each character label in intger form with the audio\n",
    "\n",
    "\n",
    "\n",
    "class TextMapping():\n",
    "\n",
    "  #create mapping via dictionary and transforming function between chars and ints\n",
    "    def __init__(self):\n",
    "\n",
    "    #initialize chars list\n",
    "        self.chars = ['\\'', ' ',\n",
    "             'a', 'b', \n",
    "             'c', 'd', \n",
    "             'e', 'f', \n",
    "             'g', 'h', \n",
    "             'i', 'j', \n",
    "             'k', 'l', \n",
    "             'm', 'n', \n",
    "             'o', 'p', \n",
    "             'q', 'r', \n",
    "             's', 't', \n",
    "             'u', 'v', \n",
    "             'w', 'x', \n",
    "             'y', 'z']\n",
    "\n",
    "        self.indexes = [i for i in range(len(self.chars))]\n",
    "        self.char_2_int = {}\n",
    "        self.int_2_char = {}\n",
    "        for idx in range(len(self.chars)):\n",
    "            self.char_2_int[self.chars[idx]] = self.indexes[idx]\n",
    "            self.int_2_char[self.indexes[idx]] = self.chars[idx]\n",
    "        \n",
    "    def textToInt(self, text_sequence): #need to define a function to convert sequence to a sequences of integers\n",
    "\n",
    "\n",
    "        sequence_number = []\n",
    "        print(text_sequence)\n",
    "        for char in text_sequence[0]:\n",
    "            char = char.lower()\n",
    "            sequence_number.append(self.char_2_int[char])\n",
    "        return sequence_number\n",
    "\n",
    "    def intToText(self, int_sequence):\n",
    "        sequence_char = []\n",
    "        for label in int_sequence:\n",
    "            sequence_char.append(self.int_2_char[label])\n",
    "\n",
    "            #build the string from list \n",
    "            text = ''.join(sequence_char)\n",
    "        return text\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "instance = TextMapping()\n",
    "text_mapping = TextMapping()\n",
    "print('aa')\n",
    "text = ['Hello My First Speech Recognition System']\n",
    "print(instance.textToInt(text))\n",
    "\n",
    "ints = instance.textToInt(text)\n",
    "print(instance.intToText(ints))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "13fa669c-6fe1-47af-a755-143766edd720",
     "kernelId": ""
    },
    "id": "YymPCT2rECGg"
   },
   "outputs": [],
   "source": [
    "#Select Subset voice data and its corresponding label data to a dir\n",
    "#Split from the subset to train and validation set with corresponding label data \n",
    "\n",
    "def train_valid_split(data_path, train_dest_path, valid_dest_path, train_set_ratio = 0.7):\n",
    "\n",
    "    data_dir = os.walk(data_path)\n",
    "  \n",
    "    train_index_lst = []\n",
    "    valid_index_lst = []\n",
    "    for path, dir_name, file_list in data_dir:\n",
    "    \n",
    "    #need to get total files in subset to calculate the number for train and valid\n",
    "        total_data = len(file_list)\n",
    "        num_train = round(total_data * 0.7)\n",
    "        num_valid = total_data - num_train\n",
    "        idx = 0\n",
    "\n",
    "    for voice_sample in file_list:\n",
    "\n",
    "        index_str = voice_sample.split('.')[0][-6:]\n",
    "        index = int(index_str)\n",
    "\n",
    "\n",
    "        sample_path = os.path.join(path, voice_sample)\n",
    "      #check the split boundary for train and validation dataset while copying and moving the data\n",
    "        dest_path = os.path.join(train_dest_path, voice_sample) if idx <= num_train else os.path.join(valid_dest_path, voice_sample)\n",
    "\n",
    "        train_index_lst.append(index) if idx <= num_train else valid_index_lst.append(index)\n",
    "        shutil.copy(sample_path, dest_path)\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    return index_lst\n",
    "\n",
    "\n",
    "def get_labels_from_CSV(csv_path):\n",
    "    with open(csv_path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "    labels = []\n",
    "    for idx in range(1, len(data)):\n",
    "        labels.append([data[idx][1]])\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "execution_count": 5,
     "id": "6f0163fe-25dc-4935-8de8-0afd22b2d16d",
     "kernelId": ""
    },
    "id": "B8yME8t4VsmD",
    "outputId": "8bef27da-1143-4883-8b2a-cb0fe2a92001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15614\n",
      "6691\n"
     ]
    }
   ],
   "source": [
    "train_labels = get_labels_from_CSV('./subset_train.csv')\n",
    "valid_labels = get_labels_from_CSV('./subset_valid.csv')\n",
    "# train_labels = get_labels_from_CSV('drive/MyDrive/ASR/subset_train.csv')\n",
    "# valid_labels = get_labels_from_CSV('drive/MyDrive/ASR/subset_valid.csv')\n",
    "print(len(train_labels))\n",
    "print(len(valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 50,
     "id": "d0446c24-2fdf-45e0-9568-d2e110dc1ceb",
     "kernelId": ""
    },
    "id": "ooWAoA9nPfJk"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from glob import glob\n",
    "#defined the dataset and how the data and label is going to be taken \n",
    "#the audio data and label should be matched\n",
    "#design how to match the data with labels from csv\n",
    "\n",
    "FRAME_SIZE = 2048\n",
    "HOP_SIZE = 512\n",
    "n_mels = 128\n",
    "\n",
    "def data_preprocessing(audio_waveform, sequence):\n",
    "    melSpec = []\n",
    "    labels = []\n",
    "    melSpec = SignalProcess.getMelspectrogram(audio_waveform, n_fft = FRAME_SIZE, hop_length = HOP_SIZE, n_mels = n_mels, fmin = 10, fmax = 8000)\n",
    "\n",
    "    melSpec = np.transpose(melSpec)\n",
    "    labels = text_mapping.textToInt(sequence)\n",
    "    return torch.Tensor(melSpec), torch.Tensor(labels)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    melspecs = []\n",
    "    labels = []\n",
    "    melspecs_length = []\n",
    "    labels_length = []\n",
    "    for melspec, label, mel_len, label_len in data:\n",
    "        melspecs.append(melspec)\n",
    "        labels.append(label)\n",
    "        melspecs_length.append(mel_len)\n",
    "        labels_length.append(label_len)\n",
    "        \n",
    "        \n",
    "    melspecs = nn.utils.rnn.pad_sequence(melspecs, batch_first = True)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first = True)\n",
    "#     print(\"Batch Mel:\\n\", melspecs)\n",
    "#     print(\"Batch Labels:\\n\",labels)\n",
    "    \n",
    "    #print(\"check bug\")\n",
    "    return melspecs, labels, melspecs_length, labels_length\n",
    "\n",
    "\n",
    "class TrainDataset(data.Dataset):\n",
    "    def __init__(self, labels,root = ''):\n",
    "        super(TrainDataset, self).__init__()\n",
    "        self.audio_files = sorted(glob(os.path.join(root, '*.wav')))\n",
    "        self.waveforms = []\n",
    "        self.text_labels = labels\n",
    "        #print(self.audio_files)\n",
    "        for audio_file in self.audio_files:\n",
    "            waveform, _ = librosa.load(audio_file)\n",
    "            self.waveforms.append(waveform)\n",
    "\n",
    "        #print('waveform:',self.waveforms)\n",
    "        #print('labels:', self.text_labels)\n",
    "\n",
    "\n",
    "\n",
    "        #apply signal processing so that get a dataset with waveform and labels correspondingly matched\n",
    "        #the transform should be converted in the init stage to cut cost while get item\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        audio_path = self.audio_files[index]\n",
    "\n",
    "        waveform = self.waveforms[index]\n",
    "\n",
    "        sequence_label = self.text_labels[index]\n",
    "\n",
    "        melSpec, labels = data_preprocessing(waveform, sequence_label)\n",
    "        \n",
    "        mel_length = melSpec.shape[0]\n",
    "        \n",
    "        label_length = len(labels)\n",
    "\n",
    "\n",
    "\n",
    "        return melSpec, labels, mel_length, label_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 51,
     "id": "32fac5fe-b8ed-459f-a8c2-d38bd97b9567",
     "kernelId": ""
    },
    "id": "j_RApjVXP8cr"
   },
   "outputs": [],
   "source": [
    "demo_train_labels = train_labels[:24]\n",
    "demo_path = './demoData/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 52,
     "id": "3438f35f-8780-459e-a42a-6b376883aece",
     "kernelId": ""
    },
    "id": "a31Gsz2EQnsw"
   },
   "outputs": [],
   "source": [
    "train_demo = TrainDataset(demo_train_labels,demo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 53,
     "id": "1780d38a-8609-4b5f-9fb8-606588a224f1",
     "kernelId": ""
    },
    "id": "pX_ROjVuQpaQ"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_demo, batch_size = 4, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "f9bc009b-27bb-44d0-a69f-be071f903808",
     "kernelId": ""
    },
    "id": "Q5MVlHEiRrP4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# #visualize some demo first to see the shape of the data in each batch\n",
    "\n",
    "\n",
    "# for idx, data in enumerate(train_loader):\n",
    "    \n",
    "#     melspec, labels = data\n",
    "#     melspec = melspec.unsqueeze(1)\n",
    "#     print(\"Batch Mel:\\n\", melspec)\n",
    "#     print(\"Batch Mel Shape:\\n\",melspec.shape)\n",
    "#     print(\"Batch Label:\\n\", labels)\n",
    "#     print(\"Batch Label shape:\\n\", labels.shape)\n",
    "#     #melspec = melspec.squeeze(1)\n",
    "    \n",
    "#     #the data size is (batch, channel, timestep, n_mels(features))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 35,
     "id": "7665a191-268f-4f58-b602-0799832a4f7d",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#define model \n",
    "#data shape (batch, channel, timestep, feature)\n",
    "\n",
    "class LayerNorm_CNN(nn.Module):\n",
    "    ''' Apply Layer Normalization for the inputs to CNN layer'''\n",
    "    def __init__(self, n_features):\n",
    "        super(LayerNorm_CNN, self).__init__()\n",
    "        self.layerNormalization = nn.LayerNorm(n_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layerNormalization(x)\n",
    "        return x #(batch, channel, timestep, features)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding = kernel // 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding = kernel // 2)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.layerNormalization_1 = LayerNorm_CNN(n_features)\n",
    "        self.layerNormalization_2 = LayerNorm_CNN(n_features)\n",
    "        self.activate = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #initialize the residual as unprocessed input x\n",
    "        residual = x #(batch, channel, timestep, features)\n",
    "        \n",
    "        #normalize the input first, apply activation and dropout, then apply weight layer\n",
    "        print(\"shape : \",x.shape)\n",
    "        x = self.layerNormalization_1(x)\n",
    "        print(\"shape : \",x.shape)\n",
    "        \n",
    "\n",
    "        x = self.activate(x)\n",
    "\n",
    "        x = self.dropout_1(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "\n",
    "        \n",
    "        x = self.layerNormalization_2(x)\n",
    "        x = self.activate(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        #skip connection \n",
    "        #add residual to output of our function \n",
    "        x += residual\n",
    "        return x \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Bidirectional_GRU(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, dropout, batch_first):\n",
    "        super(Bidirectional_GRU, self).__init__()\n",
    "        \n",
    "        self.BiGRU = nn.GRU(input_size = embedding_dim, hidden_size = hidden_size, \n",
    "                           dropout = dropout, batch_first = batch_first, bidirectional = True)\n",
    "        \n",
    "        self.layerNormalization = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #normalization first, then activation , then use the gru network to train\n",
    "        \n",
    "        print(\"input:\", x.shape)\n",
    "        x = self.layerNormalization(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x, _= self.BiGRU(x)\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 46,
     "id": "2b96ed0c-bea8-402c-801e-30009b13437f",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#thoughts of output shape\n",
    "#it is possible to build output of timestep versus chars\n",
    "#if model can get the probs of each char for each timestep\n",
    "#then the output may be (timestep, chars) or (chars, timestep)\n",
    "\n",
    "#output probs\n",
    "\n",
    "\n",
    "\n",
    "#define the combined model of cnn & rnn \n",
    "class ASR_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_convs, num_features, stride, num_filters, dropout, num_rnn, embedding_dim, num_class):\n",
    "        super(ASR_Model, self).__init__()\n",
    "        \n",
    "        num_features = num_features // 2\n",
    "        #apply one single conv to input using 32 filters to get 32 channels with more features\n",
    "        self.conv1 = nn.Conv2d(1, num_filters, kernel_size = 3, stride = stride, padding = 1) #shape should be (B, 32, H, W)\n",
    "        \n",
    "\n",
    "        self.resNet_Layers = nn.Sequential(*[\n",
    "            ResidualBlock(num_filters, num_filters, kernel = 3, stride = 1, dropout = dropout,\n",
    "                          n_features = num_features) for idx in range(num_convs)])\n",
    "        \n",
    "        #connect output of resnet to a fully-connected layer and do the forward compuataion\n",
    "        #neurons in fc then fed into rnn, so keep the dim in fc same as the feature dims of rnn\n",
    "        self.fc = nn.Linear(num_features * num_filters, embedding_dim)\n",
    "        \n",
    "        self.GRU_Layers = nn.Sequential(*[\n",
    "            Bidirectional_GRU(embedding_dim = embedding_dim if idx == 0 else embedding_dim * 2 ,hidden_size = embedding_dim,\n",
    "            dropout = dropout, batch_first = True) \n",
    "            for idx in range(num_rnn)])\n",
    "        \n",
    "        self.linear = nn.Linear(embedding_dim * 2, embedding_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_class = nn.Linear(embedding_dim, num_class)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #apply each layer to input -> output should be (timestep, class)\n",
    "        \n",
    "        \n",
    "        x = self.conv1(x) #(batch, channels, timestep, feature)\n",
    "        \n",
    "        #apply Resnet \n",
    "        x = self.resNet_Layers(x)\n",
    "        #connect layer after last resnet block to a fc layer\n",
    "        #reshape output of resnet first to put channels and features together\n",
    "        x_shapes = x.size()\n",
    "        print(x.shape)\n",
    "        x = x.view(x_shapes[0],x_shapes[2],x_shapes[1] * x_shapes[3]) #(Batch,timestep, channels * dims)\n",
    "        print(x.shape)\n",
    "        x = self.fc(x) \n",
    "        print(\"after fc\",x.shape)\n",
    "        #fc layer -> rnn model \n",
    "        x = self.GRU_Layers(x)\n",
    "        \n",
    "        #get the output from rnn layer (batch, timestep, dims)\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_class(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 54,
     "id": "9d23a33d-2cd3-4bca-8301-f4575eb85139",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "ASR_M = ASR_Model(3, 128, 2, 32, 0.2, 3, 512, 29)\n",
    "#demo_model = SpeechRecognitionModel_1(3,5,512,29,128,2,0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 55,
     "id": "528490ac-9bf3-49e2-9481-b5cbb368c4da",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"we'll pay all your expenses\"]\n",
      "['then they got ahold of some dough and went goofy']\n",
      "['it was faintly marked with transverse stripes and slightly flattened from the perfect round']\n",
      "['i have no information about troops or about the movement of the tribes']\n",
      "batch 0\n",
      "shape :  torch.Size([4, 32, 125, 64])\n",
      "shape :  torch.Size([4, 32, 125, 64])\n",
      "shape :  torch.Size([4, 32, 125, 64])\n",
      "shape :  torch.Size([4, 32, 125, 64])\n",
      "shape :  torch.Size([4, 32, 125, 64])\n",
      "shape :  torch.Size([4, 32, 125, 64])\n",
      "torch.Size([4, 32, 125, 64])\n",
      "torch.Size([4, 125, 2048])\n",
      "after fc torch.Size([4, 125, 512])\n",
      "input: torch.Size([4, 125, 512])\n",
      "input: torch.Size([4, 125, 1024])\n",
      "input: torch.Size([4, 125, 1024])\n",
      "output shape:\n",
      " torch.Size([4, 125, 29])\n",
      "the output is:\n",
      " tensor([[[ 0.1405, -0.1155,  0.1278,  ...,  0.0050, -0.2590, -0.0311],\n",
      "         [ 0.0388, -0.0646,  0.1985,  ..., -0.0790, -0.2490,  0.0309],\n",
      "         [-0.0010, -0.2183,  0.1985,  ...,  0.0554, -0.2128,  0.0516],\n",
      "         ...,\n",
      "         [ 0.0681, -0.1631,  0.2147,  ...,  0.1196, -0.2189,  0.1599],\n",
      "         [-0.0036, -0.0202,  0.1274,  ...,  0.1214, -0.1651,  0.0487],\n",
      "         [-0.0476, -0.2047,  0.1420,  ...,  0.0023, -0.1748,  0.0254]],\n",
      "\n",
      "        [[ 0.0733, -0.2937, -0.0433,  ...,  0.0277, -0.2143,  0.0252],\n",
      "         [-0.0222, -0.2077, -0.0022,  ...,  0.0767, -0.0821,  0.0483],\n",
      "         [ 0.0253, -0.1805,  0.0731,  ...,  0.0186, -0.1458,  0.0623],\n",
      "         ...,\n",
      "         [-0.0117, -0.1784,  0.1148,  ...,  0.0397, -0.0452,  0.0251],\n",
      "         [-0.0675, -0.1292,  0.0927,  ...,  0.0403, -0.0898,  0.0383],\n",
      "         [-0.0444, -0.1933,  0.1202,  ...,  0.0305, -0.2182, -0.0322]],\n",
      "\n",
      "        [[-0.0121, -0.1188,  0.1064,  ...,  0.0832, -0.1096,  0.0601],\n",
      "         [-0.0201, -0.2037,  0.1474,  ...,  0.0562, -0.1941, -0.0147],\n",
      "         [ 0.0356, -0.1902,  0.0836,  ..., -0.1127, -0.1876,  0.0439],\n",
      "         ...,\n",
      "         [-0.1045, -0.0979,  0.1991,  ...,  0.0278, -0.2191, -0.0492],\n",
      "         [-0.0085, -0.1846,  0.1787,  ..., -0.0929, -0.1410, -0.0936],\n",
      "         [-0.1114, -0.1231,  0.1511,  ...,  0.0662, -0.1246,  0.0702]],\n",
      "\n",
      "        [[ 0.0780, -0.1690,  0.0952,  ..., -0.1002, -0.2287,  0.0772],\n",
      "         [ 0.0251, -0.0895,  0.2198,  ...,  0.0071, -0.1939, -0.0560],\n",
      "         [ 0.1039, -0.0926,  0.1426,  ...,  0.0168, -0.1636, -0.0123],\n",
      "         ...,\n",
      "         [ 0.0210, -0.0680,  0.1238,  ..., -0.0227, -0.1543,  0.1171],\n",
      "         [ 0.0454, -0.0381,  0.1624,  ...,  0.0590, -0.0803, -0.0033],\n",
      "         [ 0.0074, -0.0676,  0.0504,  ...,  0.0233, -0.1421,  0.0269]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "['fill this place with a sandstorm so strong that it blots out the sun']\n",
      "['i had seen all that it would presently bring me']\n",
      "['i make my living forecasting the future for people he said']\n",
      "['i do want that platinum ring']\n",
      "batch 1\n",
      "shape :  torch.Size([4, 32, 142, 64])\n",
      "shape :  torch.Size([4, 32, 142, 64])\n",
      "shape :  torch.Size([4, 32, 142, 64])\n",
      "shape :  torch.Size([4, 32, 142, 64])\n",
      "shape :  torch.Size([4, 32, 142, 64])\n",
      "shape :  torch.Size([4, 32, 142, 64])\n",
      "torch.Size([4, 32, 142, 64])\n",
      "torch.Size([4, 142, 2048])\n",
      "after fc torch.Size([4, 142, 512])\n",
      "input: torch.Size([4, 142, 512])\n",
      "input: torch.Size([4, 142, 1024])\n",
      "input: torch.Size([4, 142, 1024])\n",
      "output shape:\n",
      " torch.Size([4, 142, 29])\n",
      "the output is:\n",
      " tensor([[[ 0.0070, -0.1365,  0.1866,  ...,  0.0053, -0.2400,  0.0057],\n",
      "         [ 0.0699, -0.1618,  0.1101,  ..., -0.0278, -0.0176,  0.0856],\n",
      "         [-0.0385, -0.2397,  0.2521,  ...,  0.0339, -0.1705,  0.1812],\n",
      "         ...,\n",
      "         [-0.0275, -0.1271,  0.1818,  ...,  0.0366, -0.2049, -0.0330],\n",
      "         [ 0.0413, -0.0715,  0.0866,  ...,  0.0120, -0.1729, -0.0466],\n",
      "         [-0.0497, -0.1384,  0.0422,  ...,  0.1163, -0.0562,  0.0427]],\n",
      "\n",
      "        [[-0.0271, -0.1752, -0.0043,  ...,  0.0250, -0.1464,  0.0236],\n",
      "         [ 0.0315, -0.1575,  0.1640,  ...,  0.0340, -0.1538,  0.0408],\n",
      "         [-0.0329, -0.1900,  0.1126,  ..., -0.0935, -0.1427,  0.1881],\n",
      "         ...,\n",
      "         [ 0.0734, -0.2131,  0.1943,  ...,  0.1366, -0.1368,  0.0515],\n",
      "         [-0.1315, -0.1700,  0.0992,  ...,  0.0581, -0.2501,  0.0723],\n",
      "         [-0.0951, -0.1622,  0.1148,  ...,  0.1457, -0.1859,  0.0199]],\n",
      "\n",
      "        [[ 0.0045, -0.1880,  0.1040,  ..., -0.0032, -0.2422,  0.0839],\n",
      "         [-0.0269, -0.1503,  0.0583,  ...,  0.0462, -0.2535,  0.0256],\n",
      "         [-0.0327, -0.2292,  0.1979,  ...,  0.1475, -0.1633,  0.1642],\n",
      "         ...,\n",
      "         [-0.0073, -0.0422,  0.0363,  ...,  0.0443, -0.2366,  0.0532],\n",
      "         [-0.0798, -0.1569,  0.1473,  ..., -0.0144, -0.1443,  0.0213],\n",
      "         [-0.0835, -0.1485,  0.0531,  ...,  0.0947, -0.0339,  0.0598]],\n",
      "\n",
      "        [[ 0.0855, -0.1765,  0.0952,  ...,  0.0415, -0.1702,  0.0236],\n",
      "         [ 0.0223, -0.0912,  0.1720,  ...,  0.1673, -0.1534,  0.0182],\n",
      "         [ 0.0325, -0.2854,  0.1396,  ...,  0.2416, -0.0655,  0.1534],\n",
      "         ...,\n",
      "         [-0.1176, -0.2562,  0.2334,  ...,  0.2036, -0.0337,  0.0448],\n",
      "         [-0.0102, -0.2711,  0.1576,  ...,  0.0344, -0.1051,  0.0595],\n",
      "         [-0.0481, -0.1330,  0.0838,  ...,  0.0157, -0.1799,  0.0552]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "['two years ago right here on this spot i had a recurrent dream too']\n",
      "[\"don't let them see that you're afraid the alchemist said\"]\n",
      "['coming home a party of tourists passed us singing and playing music']\n",
      "['before guns were invented armies had to throw bullets at each other and if a bullet touched you you had to sit out until the next war']\n",
      "batch 2\n",
      "shape :  torch.Size([4, 32, 167, 64])\n",
      "shape :  torch.Size([4, 32, 167, 64])\n",
      "shape :  torch.Size([4, 32, 167, 64])\n",
      "shape :  torch.Size([4, 32, 167, 64])\n",
      "shape :  torch.Size([4, 32, 167, 64])\n",
      "shape :  torch.Size([4, 32, 167, 64])\n",
      "torch.Size([4, 32, 167, 64])\n",
      "torch.Size([4, 167, 2048])\n",
      "after fc torch.Size([4, 167, 512])\n",
      "input: torch.Size([4, 167, 512])\n",
      "input: torch.Size([4, 167, 1024])\n",
      "input: torch.Size([4, 167, 1024])\n",
      "output shape:\n",
      " torch.Size([4, 167, 29])\n",
      "the output is:\n",
      " tensor([[[ 1.0399e-01, -1.4365e-01,  1.1485e-01,  ..., -2.9788e-02,\n",
      "          -2.2224e-01,  5.6321e-02],\n",
      "         [ 8.4805e-03, -6.3762e-02,  1.7756e-01,  ...,  4.4028e-02,\n",
      "          -2.6272e-01,  9.1660e-02],\n",
      "         [ 3.8747e-02, -1.6583e-01,  2.0888e-01,  ...,  1.2413e-01,\n",
      "          -3.0809e-01, -3.8229e-03],\n",
      "         ...,\n",
      "         [-2.9571e-02, -2.4086e-01,  7.5818e-02,  ...,  2.7679e-02,\n",
      "          -3.0371e-01,  3.1794e-02],\n",
      "         [-1.0629e-01, -1.9955e-01,  9.5048e-02,  ...,  9.5422e-02,\n",
      "          -1.7619e-01,  4.0994e-02],\n",
      "         [-1.7180e-01, -1.7587e-01,  1.0110e-01,  ...,  1.2371e-01,\n",
      "          -2.0882e-01,  2.6551e-02]],\n",
      "\n",
      "        [[ 2.3638e-02, -2.1452e-01,  7.1004e-02,  ..., -3.8771e-02,\n",
      "          -9.6215e-02,  7.8438e-02],\n",
      "         [ 4.0615e-02, -1.4127e-01,  2.4137e-01,  ...,  1.5587e-02,\n",
      "          -1.9457e-01, -6.6186e-02],\n",
      "         [ 3.2521e-02, -1.6098e-01,  1.6525e-01,  ...,  3.3316e-02,\n",
      "          -1.4209e-01,  7.7159e-02],\n",
      "         ...,\n",
      "         [-7.3468e-02, -1.2359e-01,  2.6573e-02,  ...,  7.1749e-02,\n",
      "          -2.1674e-01, -4.3680e-02],\n",
      "         [-1.3881e-01, -3.1267e-01,  2.0894e-03,  ...,  7.6083e-02,\n",
      "          -1.3986e-01,  7.7370e-02],\n",
      "         [-5.4739e-02, -1.6039e-01,  7.2664e-02,  ...,  8.8001e-02,\n",
      "          -1.1605e-01, -2.0797e-02]],\n",
      "\n",
      "        [[-9.8444e-02, -3.0229e-01,  8.0577e-02,  ...,  9.4172e-03,\n",
      "          -5.3965e-02,  8.1217e-02],\n",
      "         [ 2.3690e-02, -1.3740e-01,  1.4941e-01,  ...,  5.1227e-02,\n",
      "          -1.8621e-01,  8.5085e-02],\n",
      "         [-3.7213e-02, -1.8277e-01,  1.4779e-01,  ...,  5.8385e-02,\n",
      "          -9.2886e-02,  1.3676e-01],\n",
      "         ...,\n",
      "         [-1.1912e-04, -4.3971e-02,  1.2246e-01,  ...,  5.5147e-02,\n",
      "          -1.9830e-01,  1.0559e-02],\n",
      "         [-4.9055e-02, -1.2408e-01,  1.9821e-01,  ...,  2.9841e-02,\n",
      "          -1.7460e-01,  1.7825e-01],\n",
      "         [ 5.8568e-02, -2.0806e-01,  2.4898e-02,  ...,  1.0315e-01,\n",
      "          -1.7265e-01,  2.7562e-02]],\n",
      "\n",
      "        [[ 2.5333e-02, -1.2507e-01,  1.1930e-01,  ...,  1.1461e-01,\n",
      "          -1.3965e-01, -2.3276e-03],\n",
      "         [ 2.1422e-01, -2.2171e-01,  1.3392e-01,  ..., -8.6951e-03,\n",
      "          -1.8285e-01, -6.9074e-03],\n",
      "         [ 1.1919e-01, -1.4181e-01,  7.5693e-02,  ...,  4.4895e-02,\n",
      "          -2.1081e-01,  6.2128e-02],\n",
      "         ...,\n",
      "         [ 8.7583e-02, -1.7175e-01,  1.0622e-01,  ...,  1.0611e-01,\n",
      "          -1.4885e-01,  2.8723e-02],\n",
      "         [-2.7330e-02, -1.2424e-01,  4.8995e-02,  ..., -4.0914e-02,\n",
      "          -2.8579e-01,  2.8066e-02],\n",
      "         [-9.4273e-02, -1.3028e-01,  9.3602e-03,  ...,  4.6598e-02,\n",
      "          -1.5009e-01,  6.2412e-02]]], grad_fn=<AddBackward0>)\n",
      "['before guns were invented armies had to throw bullets at each other and if a bullet touched you you had to sit out until the next war']\n",
      "['the years of research the magic symbols the strange words and the laboratory equipment']\n",
      "['i have had the same dream twice he said']\n",
      "['the boy was strong and wanted to retaliate but he was in a foreign country']\n",
      "batch 3\n",
      "shape :  torch.Size([4, 32, 193, 64])\n",
      "shape :  torch.Size([4, 32, 193, 64])\n",
      "shape :  torch.Size([4, 32, 193, 64])\n",
      "shape :  torch.Size([4, 32, 193, 64])\n",
      "shape :  torch.Size([4, 32, 193, 64])\n",
      "shape :  torch.Size([4, 32, 193, 64])\n",
      "torch.Size([4, 32, 193, 64])\n",
      "torch.Size([4, 193, 2048])\n",
      "after fc torch.Size([4, 193, 512])\n",
      "input: torch.Size([4, 193, 512])\n",
      "input: torch.Size([4, 193, 1024])\n",
      "input: torch.Size([4, 193, 1024])\n",
      "output shape:\n",
      " torch.Size([4, 193, 29])\n",
      "the output is:\n",
      " tensor([[[ 0.0273, -0.1185,  0.0868,  ...,  0.0934, -0.1157,  0.0420],\n",
      "         [ 0.0879, -0.2227,  0.1276,  ...,  0.1407, -0.1842,  0.0189],\n",
      "         [ 0.1124, -0.1223,  0.1219,  ...,  0.1241, -0.2282, -0.0024],\n",
      "         ...,\n",
      "         [-0.0437,  0.0317,  0.1071,  ...,  0.1069, -0.1284, -0.0341],\n",
      "         [-0.0629, -0.1023,  0.0733,  ...,  0.1205, -0.0122,  0.1171],\n",
      "         [ 0.0669, -0.1392,  0.0165,  ...,  0.1214, -0.1734, -0.0479]],\n",
      "\n",
      "        [[-0.0667, -0.1723,  0.0724,  ...,  0.0502, -0.1603, -0.0310],\n",
      "         [-0.0438, -0.1710,  0.1618,  ...,  0.1091, -0.1805,  0.0162],\n",
      "         [-0.0447, -0.2659,  0.1055,  ...,  0.0897, -0.1382, -0.0783],\n",
      "         ...,\n",
      "         [ 0.0183, -0.1567,  0.1015,  ...,  0.1956, -0.2163,  0.0615],\n",
      "         [-0.0152, -0.1811,  0.1234,  ...,  0.0385, -0.1500,  0.0665],\n",
      "         [-0.1090, -0.1722,  0.1080,  ...,  0.0674, -0.2330,  0.0799]],\n",
      "\n",
      "        [[-0.0013, -0.1381,  0.0837,  ...,  0.0509, -0.2065,  0.0546],\n",
      "         [-0.0651, -0.1090,  0.0794,  ...,  0.0487, -0.1863, -0.0076],\n",
      "         [-0.0833, -0.2919,  0.0899,  ...,  0.0800, -0.2697,  0.0740],\n",
      "         ...,\n",
      "         [ 0.0100, -0.1454,  0.0643,  ...,  0.0885, -0.2562,  0.0579],\n",
      "         [-0.1005, -0.2342,  0.1631,  ...,  0.0414, -0.1541,  0.1384],\n",
      "         [-0.0428, -0.1828,  0.1046,  ...,  0.0479, -0.0942,  0.0155]],\n",
      "\n",
      "        [[ 0.0594, -0.2483,  0.1326,  ...,  0.0370, -0.1396, -0.0044],\n",
      "         [ 0.0544, -0.1878,  0.0399,  ...,  0.0556, -0.2395,  0.0032],\n",
      "         [ 0.0458, -0.2264, -0.0415,  ...,  0.0704, -0.3143, -0.0089],\n",
      "         ...,\n",
      "         [-0.1260, -0.1004,  0.1415,  ...,  0.1150, -0.1705,  0.0574],\n",
      "         [-0.0925, -0.0573,  0.0494,  ..., -0.0511, -0.1940,  0.0291],\n",
      "         [-0.1004, -0.1996,  0.0497,  ...,  0.0096, -0.1305,  0.0778]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "['ask the owner of that stall how much the sword costs he said to his friend']\n",
      "['it was dropping off in flakes and raining down on the sand']\n",
      "['the snow blasted along the side of the bridge']\n",
      "['have a good trip']\n",
      "batch 4\n",
      "shape :  torch.Size([4, 32, 117, 64])\n",
      "shape :  torch.Size([4, 32, 117, 64])\n",
      "shape :  torch.Size([4, 32, 117, 64])\n",
      "shape :  torch.Size([4, 32, 117, 64])\n",
      "shape :  torch.Size([4, 32, 117, 64])\n",
      "shape :  torch.Size([4, 32, 117, 64])\n",
      "torch.Size([4, 32, 117, 64])\n",
      "torch.Size([4, 117, 2048])\n",
      "after fc torch.Size([4, 117, 512])\n",
      "input: torch.Size([4, 117, 512])\n",
      "input: torch.Size([4, 117, 1024])\n",
      "input: torch.Size([4, 117, 1024])\n",
      "output shape:\n",
      " torch.Size([4, 117, 29])\n",
      "the output is:\n",
      " tensor([[[ 0.0358, -0.1048,  0.2109,  ...,  0.0793, -0.2101,  0.0521],\n",
      "         [ 0.0003, -0.1518,  0.1428,  ...,  0.0760, -0.1659, -0.0289],\n",
      "         [-0.0104, -0.1482,  0.2234,  ...,  0.1000, -0.2048, -0.0277],\n",
      "         ...,\n",
      "         [-0.1015, -0.2070,  0.0535,  ...,  0.1935, -0.1310, -0.0125],\n",
      "         [-0.0646, -0.1896,  0.0555,  ...,  0.1803, -0.0350, -0.0038],\n",
      "         [-0.1183, -0.1811,  0.0554,  ...,  0.1756, -0.0801,  0.0455]],\n",
      "\n",
      "        [[-0.0340, -0.1165,  0.0315,  ...,  0.0588, -0.1595, -0.0499],\n",
      "         [ 0.1545, -0.0544,  0.1065,  ...,  0.0668, -0.2185, -0.0016],\n",
      "         [ 0.1152, -0.1337,  0.1096,  ...,  0.0377, -0.3041, -0.0584],\n",
      "         ...,\n",
      "         [-0.0213, -0.1260, -0.0051,  ...,  0.0449, -0.2866,  0.0251],\n",
      "         [-0.0152, -0.1211,  0.1002,  ..., -0.0814, -0.0577,  0.0419],\n",
      "         [-0.0188, -0.1653,  0.0815,  ...,  0.0901, -0.1686,  0.1016]],\n",
      "\n",
      "        [[ 0.0804, -0.1127,  0.1039,  ...,  0.0453, -0.2564,  0.0520],\n",
      "         [ 0.1113, -0.0581,  0.1657,  ..., -0.0200, -0.2588, -0.0291],\n",
      "         [ 0.0962, -0.1609,  0.1195,  ...,  0.1008, -0.1500,  0.1446],\n",
      "         ...,\n",
      "         [-0.0473, -0.1063,  0.1159,  ...,  0.0615, -0.1571,  0.0662],\n",
      "         [ 0.0248, -0.0639,  0.1226,  ...,  0.1491, -0.1322, -0.0154],\n",
      "         [-0.0015, -0.0647,  0.0566,  ...,  0.0803, -0.2248,  0.0278]],\n",
      "\n",
      "        [[ 0.0178, -0.1895,  0.0403,  ...,  0.0508, -0.0610,  0.0184],\n",
      "         [-0.0369, -0.1841,  0.0756,  ..., -0.0450, -0.1958,  0.1152],\n",
      "         [ 0.0839, -0.1933,  0.0467,  ...,  0.0343, -0.2025,  0.0994],\n",
      "         ...,\n",
      "         [-0.0589, -0.2049,  0.0616,  ...,  0.1087, -0.1927,  0.0177],\n",
      "         [ 0.0356, -0.2241,  0.1114,  ...,  0.0837, -0.1885,  0.1129],\n",
      "         [-0.0180, -0.1634,  0.0532,  ...,  0.0516, -0.0933,  0.0803]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "[\"trust in your heart but never forget that you're in the desert\"]\n",
      "['separation of garbage makes recycling possible']\n",
      "['they were people of the desert and clamored to hear his stories about the great cities']\n",
      "['my seven year resume gap is marked not drugs']\n",
      "batch 5\n",
      "shape :  torch.Size([4, 32, 235, 64])\n",
      "shape :  torch.Size([4, 32, 235, 64])\n",
      "shape :  torch.Size([4, 32, 235, 64])\n",
      "shape :  torch.Size([4, 32, 235, 64])\n",
      "shape :  torch.Size([4, 32, 235, 64])\n",
      "shape :  torch.Size([4, 32, 235, 64])\n",
      "torch.Size([4, 32, 235, 64])\n",
      "torch.Size([4, 235, 2048])\n",
      "after fc torch.Size([4, 235, 512])\n",
      "input: torch.Size([4, 235, 512])\n",
      "input: torch.Size([4, 235, 1024])\n",
      "input: torch.Size([4, 235, 1024])\n",
      "output shape:\n",
      " torch.Size([4, 235, 29])\n",
      "the output is:\n",
      " tensor([[[ 0.0517, -0.2141,  0.0825,  ..., -0.0106, -0.1202, -0.0083],\n",
      "         [-0.0272, -0.1928,  0.1273,  ...,  0.1115, -0.1329, -0.0336],\n",
      "         [ 0.0139, -0.1350,  0.2142,  ...,  0.1122, -0.0659,  0.1013],\n",
      "         ...,\n",
      "         [ 0.0413, -0.0599,  0.0654,  ...,  0.1420, -0.1862, -0.0476],\n",
      "         [-0.0079, -0.1163,  0.1092,  ...,  0.2055, -0.2587,  0.0106],\n",
      "         [-0.0138, -0.1538,  0.1096,  ...,  0.1368, -0.1439,  0.0399]],\n",
      "\n",
      "        [[ 0.0034, -0.1178,  0.0234,  ...,  0.0088, -0.2266,  0.1022],\n",
      "         [ 0.0527, -0.1295,  0.0544,  ..., -0.0356, -0.2033, -0.0137],\n",
      "         [-0.0757, -0.2441,  0.0384,  ...,  0.0706, -0.1441,  0.0686],\n",
      "         ...,\n",
      "         [-0.0928, -0.1448,  0.0973,  ..., -0.0086, -0.1733, -0.0582],\n",
      "         [ 0.0059, -0.1280,  0.2056,  ...,  0.0651, -0.1620,  0.0795],\n",
      "         [-0.1488, -0.2541,  0.1319,  ...,  0.0966, -0.1109,  0.0028]],\n",
      "\n",
      "        [[ 0.0110, -0.0949,  0.0969,  ...,  0.0747, -0.1953,  0.0339],\n",
      "         [ 0.0320, -0.0923,  0.1773,  ...,  0.0090, -0.1460,  0.0283],\n",
      "         [ 0.0512, -0.2736,  0.2239,  ...,  0.1580, -0.1749,  0.1080],\n",
      "         ...,\n",
      "         [-0.0537, -0.2134,  0.0444,  ..., -0.0687, -0.2478,  0.0849],\n",
      "         [-0.0230, -0.1512, -0.0274,  ..., -0.0551, -0.1608,  0.1081],\n",
      "         [-0.0019, -0.1493,  0.0293,  ...,  0.1176, -0.1204,  0.2104]],\n",
      "\n",
      "        [[ 0.0750, -0.1354,  0.1526,  ...,  0.0389, -0.2162,  0.0296],\n",
      "         [ 0.1363, -0.2077,  0.1057,  ..., -0.0835, -0.2440, -0.0022],\n",
      "         [ 0.0117, -0.1043,  0.1666,  ..., -0.1079, -0.2831,  0.0246],\n",
      "         ...,\n",
      "         [-0.0631, -0.1340,  0.2794,  ...,  0.1032, -0.1169,  0.0727],\n",
      "         [-0.0594, -0.1655,  0.0603,  ...,  0.0215, -0.2579, -0.0339],\n",
      "         [-0.0570, -0.2498, -0.0051,  ...,  0.1536, -0.2049, -0.0053]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for idx, data in enumerate(train_loader):\n",
    "    \n",
    "    print(\"batch\", idx)\n",
    "    melspec, labels, mel_len, label_len = data\n",
    "    melspec = melspec.unsqueeze(1)\n",
    "#     print(\"Batch Mel:\\n\", melspec)\n",
    "#     print(\"Batch Mel Shape:\\n\",melspec.shape)\n",
    "#     print(\"Batch Label:\\n\", labels)\n",
    "#     print(\"Batch Label shape:\\n\", labels.shape)\n",
    "    #melspec = melspec.squeeze(1)\n",
    "    \n",
    "    x = ASR_M(melspec)\n",
    "\n",
    "    print(\"output shape:\\n\", x.shape)\n",
    "    print(\"the output is:\\n\", x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "3cc181e1-c996-421a-b7ca-cd8c598853ac",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "train_loader = DataLoader(dataset = train_demo, batch_size = 4, collate_fn = collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 57,
     "id": "508ed4b2-3809-41a0-814f-044153784103",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "#define training process\n",
    "#train data in train_loader\n",
    "def train(model, epochs, device, train_data, loss_func, optimizer, scheduler):\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        \n",
    "        print(\"Current EPOCH:\", epoch)\n",
    "    \n",
    "        for idx, data in enumerate(train_data):\n",
    "\n",
    "            melspec, labels, melspectrogram_length, labels_length = data\n",
    "            melspec = melspec.unsqueeze(1)\n",
    "            melspec, labels = melspec.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad() #initialize gradients\n",
    "\n",
    "            output_probs_mat = ASR_M(melspec)\n",
    "            output_probs_mat = F.log_softmax(output_probs_mat, dim = 2)\n",
    "\n",
    "            #use output prob matrix to calculate CTC LOSS\n",
    "            loss = loss_func(output_probs_mat, labels, melspectrogram_length, labels_length)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            print(f'EPOCH: {epoch} | Batch: {idx}  | loss: {loss.item()}')\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            \n",
    "        \n",
    "  \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "2d2945cd-1d70-4672-a594-578e7a3b126e",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "afec14d7-f41f-40bd-9b7a-aba93501dfe4",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "60ff036c-31af-47c7-b230-1a590814b2ec",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "ee55fcff-775f-43fb-aaa3-dc198bbcb6a1",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "63e3a79d-7ec9-443e-9841-0a1a4cf37c02",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "457047f7-7e9a-4dc9-b71e-12577e8dc058",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 65,
     "id": "4249691e-952c-4fde-8e62-4700bc26618b",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "6e02eef8-91ac-4ea8-ae09-44d7f906b3ad",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "7cc0173e-2eb8-4ee9-b50e-205e60926cf4",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "6f608ad7-00ee-4f9c-be75-5eda359adf54",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "afb12c1a-d5af-4c24-9ede-9e8b20d0ef71",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ASR_Model.ipynb",
   "provenance": []
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
